% !TEX root=./tech-specification.tex

\section{Multi-point Evaluation Hashing}
\label{app:mp_eval_hash}

\subsection{Definitions}
We employ the following definitions:

\begin{tabu*}{lcl}
\toprule
Name & Value & Description \\
\midrule
\linkdest{J__wordbytes}{}$J_{wordbytes}$ & $4$  & Bytes in word. \\
\linkdest{J__datasetinit}{}$J_{datasetinit}$ & $3\times 2^{31}$ & Bytes in dataset at genesis. \\
\linkdest{J__datasetgrowth}{}$J_{datasetgrowth}$ & $2^{24}$ & Dataset growth per stage. \\
\linkdest{J__cacheinit}{}$J_{cacheinit}$ & $3\times 2^{23}$ & Bytes in cache at genesis. \\
\linkdest{J__cachegrowth}{}$J_{cachegrowth}$ & $2^{16}$ & Cache growth per stage. \\
\linkdest{J__stage}{}$J_{stage}$ & $2^{19}$ & Epoches per stage. \\
\linkdest{J__mixbytes}{}$J_{mixbytes}$ & $128$ & mix length in bytes. \\
\linkdest{J__hashbytes}{}$J_{hashbytes}$ & $64$ & Hash length in bytes. \\
\linkdest{J__parents}{}$J_{parents}$ & $256$ & Number of parents of each dataset element. \\
\linkdest{J__cacherounds}{}$J_{cacherounds}$ & $3$ & Number of rounds in cache production. \\
\linkdest{J__accesses}{}$J_{accesses}$ & $64$ & Number of accesses in hashimoto loop. \\
\bottomrule
\end{tabu*}

\subsection{Size of dataset and cache}
The size for the hash function's cache $\mathbf{c} \in \B^*$  and dataset $\mathbf{d} \in \B^*$ depend on the stage, which in turn depends on the block height $\head_{h}$.
\begin{equation}
 E_{stage}(\head_{h}) \eqdef \left\lfloor\frac{\head_{h}}{J_{stage}}\right\rfloor
\end{equation}
The size of the dataset growth by $J_{datasetgrowth}$ bytes, and the size of the cache by $J_{cachegrowth}$ bytes, every stage. In order to avoid regularity leading to cyclic behavior, the size must be a prime number. Therefore the size is reduced by a multiple of $J_{mixbytes}$, for the dataset, and $J_{hashbytes}$ for the cache.
\linkdest{d__size}{}Let $d_{size} = \lVert\dataset \rVert$ be the size of the dataset, 
which is calculated using
\begin{equation}
 d_{size} \eqdef E_{\mathrm{prime}}(J_{datasetinit} + J_{datasetgrowth} \cdot E_{stage} - J_{mixbytes}, J_{mixbytes})
\end{equation}
The size of the cache, $c_{size}$, is calculated using
\begin{equation}
 c_{size} \eqdef E_{\mathrm{prime}}(J_{cacheinit} + J_{cachegrowth} \cdot E_{stage} - J_{hashbytes}, J_{hashbytes})
\end{equation}
where $E_{\mathrm{prime}}(x, y)$ computes the greatest integer $x'$ below $x$ such that $x'/y$ is a prime (given that $x\gg y$).
\begin{equation}
	E_{\mathrm{prime}}(x, y) \eqdef 
	\begin{cases}
		x & \text{if $x / y$ is prime} \\
		E_{\mathrm{prime}}(x - 2 \cdot y, y) & \text{if $x / y$ is not prime but $x / y \in \N$} \\
		% 0 & \text{if $x / y<0$ (this case should not happen on valid input $x,y$)} \\
		E_{\mathrm{prime}}(\lfloor x/y \rfloor\cdot y, y) & \text{otherwise ($x$ is rounded to a multiple of $y$)}
	\end{cases}
\end{equation}
\subsection{Dataset generation}
In order to generate the dataset we need the cache $\mathbf{c}$, which is an array of bytes. It depends on the cache size  $c_{size}$ and the seed hash $\mathbf{s} \in \B_{256}$.
\subsubsection{Seed hash}
The seed hash is different for every stage. For the first stage it is the Keccak-256 hash of a series of 256 bits (32 bytes) of zeros. For every other stage it is always the Keccak-256 hash of the previous seed hash:
\begin{equation}
 \mathbf{s} \eqdef C_{seedhash}(\head_{h})
\end{equation}
\begin{equation}
 C_{seedhash}(\head_{h}) \eqdef \begin{cases}
\mathbf{0}_{256} & \text{if} \quad E_{stage}(\head_{h}) = 0 \quad  \\
\kec(C_{seedhash}(\head_{h} - J_{stage})) & \text{otherwise}
\end{cases}
\end{equation}
where $\mathbf{0}_{256}$ denotes $256$ bits of zeros.

\subsubsection{Cache}
The cache production process involves using the seed hash to first sequentially filling up $c_{size}$ bytes of memory, then performing $J_{cacherounds}$ passes of the RandMemoHash algorithm created by \cite{lerner2014randmemohash}. The initial cache $\mathbf{c'}$ will be constructed as follows.

Recalling that $\kec512$ denotes the Keccak-512 hash function whose output length is $512$ bits ($64$ single bytes), 
we define the array $\mathbf{c}_{i}$ as the $i$-th element of the initial cache:
\begin{equation}
	\mathbf{c}_{i} \eqdef 
	\begin{cases}
		\kec512(\mathbf{s}) & \text{if} \quad i = 0 \quad  \\
		\kec512(\mathbf{c}_{i-1}) & \text{otherwise}
	\end{cases}
\end{equation}

Let $n$ denote the number of elements in cache:
\begin{equation}
	n \eqdef \left\lfloor c_{size}/J_{hashbytes}\right\rfloor
\end{equation}
The initial cache $\mathbf{c'}$ can be defined as:
\begin{equation}
 \mathbf{c'}[i] \eqdef \mathbf{c}_{i}, \quad \forall \quad i \in\set{0,1,2,\dots, n-1}
\end{equation}


The cache $\mathbf{c}$,
consisting of $n$ items of $\kec512$ hash values, 
is calculated by performing $J_{cacherounds}$ rounds of the RandMemoHash algorithm to the initial cache $\mathbf{c'}$:
\begin{equation}
 \mathbf{c} \eqdef E_{cacherounds}(\mathbf{c'}, J_{cacherounds})
\end{equation}
\begin{equation}
	E_{cacherounds}(\mathbf{x}, y) \eqdef 
	\begin{cases}
		\mathbf{x} & \text{if} \quad y = 0 \quad  \\
		E_\text{\tiny RMH}(\mathbf{x}) & \text{if} \quad y = 1 \quad  \\
		E_{cacherounds}(E_\text{\tiny RMH}(\mathbf{x}), y -1 ) & \text{otherwise}
	\end{cases}
\end{equation}

Every single round of the RandMemoHash algorithm modifies each subset of the cache as follows:
\begin{equation}
	E_\text{\tiny RMH}(\mathbf{x}) \eqdef \big( E_{rmh}(\mathbf{x}, 0), E_{rmh}(\mathbf{x}, 1), ... , E_{rmh}(\mathbf{x}, n - 1) \big)\linkdest{E__cacherounds}{}
\end{equation}
\begin{multline}
	E_{rmh}(\mathbf{x}, i) \eqdef \kec512(\mathbf{x'}[(i - 1 + n) \mod n] \oplus \mathbf{x'}[\mathbf{x'}[i][0] \mod n]) \\
	\text{with} \quad \mathbf{x'} = \mathbf{x} \quad \text{except} \quad \mathbf{x'}[j] = E_{rmh}(\mathbf{x}, j), \quad \forall \quad j < i
\end{multline}

\subsubsection{Full dataset calculation} \label{app:dataset}
Essentially, we combine data from $J_{parents}$ pseudorandomly selected cache nodes, and hash that to compute the dataset. The entire dataset is then generated by a number of items, each of $J_{hashbytes}$ bytes in size:
\begin{equation}
\dataset[i] \eqdef E_{datasetitem}(\mathbf{c}, i), \quad \forall \quad i < \left\lfloor\frac{d_{size}}{J_{hashbytes}}\right\rfloor
\end{equation}
In order to calculate the single item we use an algorithm $E_\text{\tiny FNV}:\B_{32}\times\B_{32} \to \B_{32}$ inspired by the FNV hash \cite{FowlerNollVo1991FNVHash} in some cases as a non-associative substitute for XOR.
\begin{equation}
	E_\text{\tiny FNV}(\mathbf{x}, \mathbf{y}) \eqdef \left( (\mathbf{x} \times \mathrm{0x01000193}) \oplus \mathbf{y}\right) \mod 2^{32}
\end{equation}
The single item of the dataset can now be calculated by iteratively mixing items from the cache $\mathbf{c}$ as follows:
\begin{equation}
 E_{datasetitem}(\mathbf{c}, i) \eqdef 
 \kec512\Big(E_{parents}\big(\mathbf{c}, i, 0, \kec512(\mathbf{c}[i \mod n ] \oplus i)\big)\Big)
\end{equation}
\begin{equation}
  E_{parents}(\mathbf{c}, i, p, \mathbf{m}) \eqdef \begin{cases}
E_{parents}\big(\mathbf{c}, i, p +1, E_{mix}(\mathbf{m}, \mathbf{c}, i, p)\big) & \text{if} \quad p < J_{parents} -1 \\
E_{mix}(\mathbf{m}, \mathbf{c}, i, p) & \text{otherwise}
\end{cases}
\end{equation}
\begin{equation}
 E_{mix}(\mathbf{m}, \mathbf{c}, i, p) \eqdef E_\text{\tiny FNV}^*\Big(\mathbf{m}, \mathbf{c}[E_\text{\tiny FNV}(i \oplus p, ~ \mathbf{m}\big[p \mod \lfloor J_{hashbytes} / J_{wordbytes} \rfloor ]) \mod n \big] \Big)
\end{equation}
where $\mathbf{m}\in\B_{512}$ should be interpreted as an array of $\lfloor J_{hashbytes} / J_{wordbytes} \rfloor$ words (each of $J_{wordbytes}$ bytes, i.e. $32$ bits),
and $E_\text{\tiny FNV}^*$ denotes the element-wise invocation of $E_\text{\tiny FNV}$.

\subsection{Proof-of-work function}
\label{appsec:pow}

Essentially, we maintain a ``mix'' of $J_{mixbytes}$ bytes wide, and repeatedly sequentially fetch $J_{mixbytes}$ bytes from the full dataset and use the $E_\text{\tiny FNV}$ function to combine it with the mix. 
$J_{mixbytes}$ bytes of sequential access are used so that each round of the algorithm always fetches a full page from RAM, minimizing translation lookaside buffer misses which ASICs would theoretically be able to avoid.

If the output of this algorithm is below the desired target, then the nonce is valid. Note that the extra application of $\kec$ at the end ensures that there exists an intermediate nonce which can be provided to prove that at least a small amount of work was done; this quick outer PoW verification can be used for anti-DDoS purposes. It also serves to provide statistical assurance that the result is an unbiased, 256 bit number.

The $\mpethash$ function takes $\headernon$, which is the hash of the header excluding the {\bf mixHash} and  {\bf nonce} fields,
i.e. $\headernon\eqdef \kec\left(\rlp( \head_{-n} )\right)$,
together with the nonce $\head_{n}$
and the dataset $\dataset$ from \cref{app:dataset} as input.
The output of $\mpethash$ consists of an array with the compressed mix $\compressedmix$ as its first item and the Keccak-256 hash of the concatenation of the seed hash $\seedhash\in \B_{512}$, the compressed mix $\compressedmix\in \B_{256}$,
and the compressed multi-point mix $\mpmix\in\B_{32}$ as the second item:
\begin{equation}
	\mpethash(\headernon, \head_{n},\dataset) \eqdef 
	\set{\compressedmix, \kec\left(\seedhash \circ \compressedmix \circ \mpmix \right)}
\end{equation}

\subsubsection{Seed hash}
The seed hash $\seedhash\in \B_{512}=\Byte_{J_{hashbytes}}$ is defined on $\headernon$ and $\head_{n}$ as follows:
\begin{equation}
 \seedhash \eqdef 
 \seedhash(\headernon, \head_{n}) \eqdef \kec512\left(\headernon \circ E_{revert}(\head_{n})\right)
\end{equation}
where $E_{revert}(\head_{n})$ returns the reverted bytes sequence of the nonce $\head_{n}$ as $E_{revert}(\head_{n})[i] \eqdef \head_{n}\left[~\lVert \head_{n} \rVert -i~\right]$,
so that the $64$-bit nonce in big-endian order is changed to little-endian representation.

\subsubsection{Compressed mix}
The compressed mix $\compressedmix \in \B_{256}$ is obtained from the seed hash $\seedhash$ and the dataset $\dataset$:
\begin{equation}\label{eq:compress}
 \compressedmix \eqdef 
 \compressedmix\big(\seedhash,\dataset) \eqdef E_{compress}(E_{accesses}(\dataset, \mathbf{m}_0, \seedhash, 0), 0\big)
\end{equation}
where $\mathbf{m}_0$ and $E_{accesses}$, $E_{compress}$ are defined as follows.

The initial mix $\mathbf{m}_0$ is an array of $\lfloor J_{mixbytes} / J_{wordbytes} \rfloor$ many $32$-bit (i.e. $4$-byte) words 
obtained by replicating the seed hash $\seedhash$ for $n_{mix}$ times, 
with $n_{mix}$ defined as:
\begin{equation}
	n_{mix}\eqdef  \left\lfloor \frac{J_{mixbytes}}{J_{hashbytes}}\right\rfloor
\end{equation}
Formally, the initial mix 
$\mathbf{m}_0 \in \B_{512 \cdot n_{mix}}
=\left(\Byte_{J_{hashbytes}}\right)^{n_{mix}}$ is defined as:
\begin{equation}
	\mathbf{m}_0 \eqdef \underbrace{\seedhash \circ \cdots \circ \seedhash}_{\text{$n_{mix}$ many copies of $\seedhash$}}
\end{equation}

In order to add random dataset nodes to the mix, the $E_{accesses}$ function is used:
\begin{equation}
 	E_{accesses}(\mathbf{d}, \mathbf{m}, \mathbf{s}, i) 
 	\eqdef 
 	\begin{cases}
		E_{mixdataset}(\mathbf{d}, \mathbf{m},  \mathbf{s}, i) & \text{if} \quad i = J_{accesses} -1 \\
		E_{accesses}\big(\dataset, E_{mixdataset}(\mathbf{d}, \mathbf{m}, \mathbf{s}, i), \mathbf{s}, i + 1\big) & \text{otherwise}
	\end{cases}
\end{equation}
\begin{equation}
 E_{mixdataset}(\mathbf{d}, \mathbf{m}, \mathbf{s}, i) \eqdef E_\text{\tiny FNV}^*\big(\mathbf{m}, E_{newdata}(\mathbf{d}, \mathbf{m}, \mathbf{s}, i)\big)
\end{equation}
where $E_\text{\tiny FNV}^*$ denotes the element-wise invocation of $E_\text{\tiny FNV}$ on $32$-bit elements,
and $E_{newdata}$ returns an array consisting of $n_{mix}$ elements from the dataset $\dataset$ (so that the array is of same length as the intermediate mix $\mathbf{m}$, i.e. each of $J_{mixbytes} = n_{mix}\cdot J_{hashbytes}$ bytes):
\begin{equation}
 	E_{newdata}(\mathbf{d}, \mathbf{m}, \mathbf{s}, i)[j] 
 	\eqdef \dataset\left[ {p} \cdot n_{mix} + j\right], \quad \forall \quad 0\le j \le n_{mix}-1
\end{equation}
with the mixing index ${p}$ obtained as follows:
\begin{equation}
	{p} \eqdef 
	E_\text{\tiny FNV}\left(i \oplus \mathbf{s}[0], \mathbf{m}\left[i \mod \left\lfloor\frac{J_{mixbytes}}{J_{wordbytes}}\right\rfloor\right]\right) \mod \left\lfloor\frac{d_{size} / J_{hashbytes}}{n_{mix}}\right\rfloor
\end{equation}

The $E_{compress}$ function converts $J_{mixbytes}$-byte mix eventually returned by $E_{accesses}$, 
which is an array of $\lfloor J_{mixbytes} / J_{wordbytes} \rfloor$ words,
into the compressed mix $\compressedmix \in \Byte_{32}=\Byte_{\lfloor J_{mixbytes}/4 \rfloor}$ as follows:
\begin{equation}
	E_{compress}(\mathbf{m}, i) \eqdef 
	\begin{cases}
		\emptystring & \text{if} \quad i \geqslant \lVert \mathbf{m} \rVert - 4 \\
		E_\text{\tiny FNV}(E_\text{\tiny FNV}(E_\text{\tiny FNV}(\mathbf{m}[i], \mathbf{m}[i + 1]), \mathbf{m}[i + 2]), \mathbf{m}[i + 3]) 
		\circ E_{compress}(\mathbf{m}, i + 4) & \text{otherwise}
	\end{cases}
\end{equation}

\subsubsection{Multi-point mix}
The multi-point mix $\mpmix\in \B_{32}$ is calculated from the header $\headernon$
and $\seedhash$, $\dataset$ as follows:
\begin{equation}
	\mpmix \eqdef \mpmix\left( \headernon, \seedhash, \dataset \right)
	\eqdef  E_{\text{\tiny FNV}-compress}\left(E_{mp-eval}(a,b,c,w,\vec{z}),J_{accesses} \right)
\end{equation}
with arguments and functions described in the rest of this section.

Interpreting $\headernon$ as a byte array, 
input arguments $a,b,c,w\in \B_{32}$ are defined as follows:
\begin{align}
	a &\eqdef \headernon[0]\\
	b &\eqdef \headernon[1]\\
	c &\eqdef \headernon[2]\\
	w &\eqdef \headernon[3]
\end{align}
whereas the last argument $\vec{z}$ is an array of $J_{accesses}$ items drawn from $\B_{32}$.
More specifically, $\vec{z}[i]$ is defined as:
\begin{equation}
	\vec{z}[i] \eqdef 
	E_{parity}\left(E_{mp-accesses}(\mathbf{d}, \mathbf{m}_0, \seedhash, i, 0)\right),
	\quad \forall \quad 0\le i \le J_{accesses}-1
\end{equation}
where the $E_{mp-accesses}$ function is defined as:
\begin{equation}
	E_{mp-accesses}(\mathbf{d}, \mathbf{m}, \mathbf{s}, i, j)
	\eqdef 
 	\begin{cases}
		E_{mixdataset}(\mathbf{d}, \mathbf{m},  \mathbf{s}, i) & \text{if} \quad j \ge i \\
		E_{mp-accesses}\big(\dataset, E_{mixdataset}(\mathbf{d}, \mathbf{m}, \mathbf{s}, j), \mathbf{s}, i, j + 1\big) & \text{otherwise}
	\end{cases}
\end{equation}
and the $E_{parity}$ function is defined over $\Byte_{J_{mixbytes}} \to \B_{32}$ and 
it simply returns the parity of all items in the input array at word level (i.e. realizing $\Byte_{J_{mixbytes}}$ as ${\lfloor J_{mixbytes}/J_{wordbytes}\rfloor}$ elements each of $J_{wordbytes}$ bytes),  
formally defined as:
\begin{equation}
	E_{parity}(\vec{m}) \eqdef \oplus_{i=0}^{\lfloor J_{mixbytes}/J_{wordbytes}\rfloor -1} \vec{m}[i]
\end{equation}

The $E_{mp-eval}$ function evaluates the polynomial $E_{polynomial}$ on multiple points $\vec{x}[i]$ for $i\in \set{0,1,\dots, J_{accesses}-1}$ and returns the concatenation of $J_{accesses}$ elements drawn from $\B_{32}$ as follows:
\begin{equation}
	E_{mp-eval}(a,b,c,w,\vec{z}) \eqdef 
	E_{polynomial}\left(\vec{x}[0]\right) \circ E_{polynomial}\left(\vec{x}[1]\right) \circ \cdots \circ
	E_{polynomial}\left(\vec{x}[J_{accesses-1}]\right)
\end{equation}
where $\vec{x}$ is an array of $32$-bit integers defined as:
\begin{equation}
	\vec{x}[i] \eqdef a+b\cdot w^i + c\cdot w^{2i}, \quad \forall \quad 0\le i \le J_{accesses}-1
\end{equation}
and the $E_{polynomial}$ function is a polynomial with coefficients specified by $\vec{z}$:
\begin{equation}
	E_{polynomial}(x) \eqdef \left(\sum_{j=0}^{J_{accesses}-1} \vec{z}[j]\cdot x^j\right) \mod (10^9+7)
\end{equation}

The FNV-compress function $E_{\text{\tiny FNV}-compress}$ is defined over $\left(\B_{32}\right)^* \times \N \to \B_{32}$ 
and used to compress an array of $\B_{32}$ elements into a single $\B_{32}$ element:
\begin{equation}
	E_{\text{\tiny FNV}-compress}(\vec{m},i) \eqdef 
	\begin{cases}
		\vec{0}_{32} & \text{if} \quad i < 1 \\
		E_\text{\tiny FNV}\left( 
		E_{\text{\tiny FNV}-compress}(\vec{m},i-1) ,
		\vec{m}[i-1]\right)
		& \text{otherwise}
	\end{cases}
\end{equation}
